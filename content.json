{"meta":{"title":"Peter Toonen on all things tech","subtitle":null,"description":"All about tech and the adventures of Peter Toonen in the digital world.","author":"Peter Toonen","url":"https://toonen.io"},"pages":[],"posts":[{"title":"It's been a while","slug":"It-s-been-a-while","date":"2019-03-19T09:30:41.000Z","updated":"2019-03-19T19:14:20.545Z","comments":true,"path":"2019/03/19/It-s-been-a-while/","link":"","permalink":"https://toonen.io/2019/03/19/It-s-been-a-while/","excerpt":"","text":"It’s been a while since I’ve posted on this blog. There are several reasons for this. First one being that I’ve always gotten more energy from (public) speaking rather than writing. The second being that I actually really needed to conserve my energy for about a year or so - and actually still do. It all started this with this: One of the cars involved in this accident was mine. Whilst driving to a friend, an accident happened just in front of me. All three of the lanes came to an immediate halt. From 130 km/h to 0. I always steer towards the side of the road in these situations and that allowed the van driving behind me to avoid a collision. He came to a halt with the nose of his vehicle close to my passenger door. Unfortunately the driver operating the van behind him, wasn’t paying attention to the road and hit me from behind while driving about 120-130 km/h in his fully loaded Mercedes Sprinter. At first I was fineImmediately after the accident, I seemed to be just fine. My leg was hurting a little from hitting the steering wheel, but other than that I seemed fine. Although I knew I’d have a muscle ache the day after, I counted my blessings and went home. That night, I celebrated the fact that I’d walked out of a severe accident relatively unscathed. The day after, I did indeed have severely painful muscles but if that was the worst of it, I still wasn’t too worried. But then… I remember waking up on Sunday the 1st of April 2018 having a really bad neck pain and I was dizzy. I figured I had slept in a weird position and that this would pass. Unfortunately it didn’t. Not the next day, not during the week after, nor the 9 months that followed. On Tuesday I went into the office and everything was still spinning. My colleagues sent me home. After a visit to a doctor and physical therapist, I was diagnosed with a concussion and a whiplash. … and then I wasn’tThis all meant that I had to take a lot of rest. For those who know me a little, it shouldn’t come as a surprise that this wasn’t easy. I am not one to sit still, but now I was forced to. I couldn’t work (wasn’t allowed by my employer either), couldn’t read, watch tv, look at a screen and low- or high-pitched sounds made my head spin like nothing else. I ended up sleeping with ear plugs and even then it was hardly doable which in turn worsened my condition. Even though I knew that eventually all would be well, it took way too long for my liking. So I developed a new hobby: I planted chili seeds and watched them grow. And seeing as doing things half baked isn’t really my thing (also, did I mention that I had a lot of time?), I ended up harvesting about 5 kg’s of chili’s. It was so bad that the whole living room was filled with chili plants :-) Meanwhile I was slowly re-integrating at work. I started out with 4x 30 mins spread out over the day and slowly worked my way up to 4 hours a day. As soon as I could work 1 hour in a row (this took me several weeks), I decided it was also time to go into the office and slowly worked my way back to fulltime (40 hours/week) working. So basically you were on holiday for about a year?Not really. It hasn’t always been easy. Just like that van went from 130 km/h to 0, so did I. There have been times where I was really down and didn’t think it would ever get better. But besides my chili’s, my dog, my girlfriend (in no particular order), there was another thing that kept me going and enabled me to slowly stretch out my days: community. Right after my accident, I couldn’t do much but I had already made a commitment to mentor some students, I had already planned some workshops (DevOps principles combined with Chaos Engineering with students), I had some speaking engagements, events, etc. and even though I didn’t think these things would go well, I noticed I got an enourmous amount of energy from it. Mentoring, teaching, speaking, engaging in conversations, that is what makes me get out of my bed, both in good times as in the less-than-good times. Are you okay now?Yes I am. I’m doing great. Unfortunately this doesn’t mean that I’m the same as before. I still struggle with long days - I still get headaches when I go on for too long. I get dizzy when I sleep too little and I have a lot less energy than I used to. But I like to think in possibilities and when I look back at the first weeks and compare that to now, I’ve come a long way. If anyone had told me before (and people have), I wouldn’t have believed that a ‘simple’ accident with so little visible damage, could have such an impact on someone’s life. So I guess I’ve also learned from this experience: patience, relaxation, but especially to appreciate the little things in life and not complain so much ;-) I’m looking forward to the next year where things will be stabilizing and I can pick up where I left off. I also hope to see you out there :)","categories":[],"tags":[{"name":"Personal","slug":"Personal","permalink":"https://toonen.io/tags/Personal/"}],"keywords":[]},{"title":"Creating a GitHub badge with Azure Functions","slug":"creating-a-github-badge-with-azure-functions","date":"2018-03-04T08:52:47.000Z","updated":"2018-03-04T18:29:56.000Z","comments":true,"path":"2018/03/04/creating-a-github-badge-with-azure-functions/","link":"","permalink":"https://toonen.io/2018/03/04/creating-a-github-badge-with-azure-functions/","excerpt":"","text":"Recently, I spent some time with the guys from the Stryker Mutator team. First in a hackathon over a weekend back in December last year, then finalizing our work in February and launching the Mutation Score Badge. Even though I had to overcome my fear of JavaScript, I managed to find some good parts in NodeJS and combine them into a Azure Function that provides the actual mutation badge. Get the why, how and what in this post. Why Azure Functions?Well, simply put: because it’s cheap, easy and it supports multiple languages. Since Stryker is writting in NodeJS, I decided to challenge myself and write the function in NodeJS as well. Our setup is quite simple: We use an Azure Storage Table to score all mutation scores posted from the Dashboard. When someone requests a badge, the function performs a lookup in this table and presents the badge We have a Function Proxy to be able to use our own domain and a friendly URL. How we developed the functionAll code was written in TypeScript and using this excellent post by Tsuyoshi Ushio, I was able to develop and debug it on my mac quite easily (well, after a crashcourse in TypeScript from Nico Jansen). Is it really all that awesome?No, it isn’t. We hit quite a few snags while developing but especially when deploying. As you read before, the functions are dirt-cheap on a consumption plan but this also means that they’re not ‘Always-On’. Where this doesn’t really seem to be an issue for regular C# functions, for some reason the NodeJS functions were extremely slow and on top of that, I had to use Kudu to do an npm install. Azure and NodeJSWe soon discovered that uploading a lot of small files to Azure would take a while, and we decided we’d just want to upload the package.json and run npm install on Azure through Kudu. Notice that on this page it also says that the Node version is locked on 6.5.0. Even adjusting the WEBSITE_NODE_DEFAULT_VERSION environment variable didn’t work. This limited us in our ability to use certain Node functionality that required version 8+ (util.promisify in particular) so we went to look for another solution. This present itself in the portal. If you look carefully at the screenshot above, you can see a variable called FUNCTIONS_EXTENSION_RUNTIME. This is set to ~1 by default, but you can simply change that to run on the ‘beta’ version. Mind you: you can only safely change this if you don’t currently have any functions deployed. Unfortunately, it turned out that changing this to the beta doesn’t support proxies yet, so we reverted and included our own promisify. Cold BootAs mentioned before, we initially planned on deploying through Kudu and simply running NPM install there and we did. Thing is, the functions were really slow. I mean… REALLY slow. It took over 20 seconds to start and as it turns out, we weren’t the only ones. Our solution was to apply FuncPack and by simply running this before our publish: 12npm install -g azure-functions-packfuncpack pack ./ we were able to pack it all into one file. What it does is that it applies WebPack magic to your function (also rewriting your function.json to reference to index.js as entrypoint). Running this brought our cold boot down to acceptable levels. What now?Well, we’re live :) There’s still some work to do by the functions team, but with the newly announced Run-From-Zip functionality, I’m positive that it’ll run even smoother than now. On top of that, we now also know what it has cost us over the month of February: a whopping $0.33 :-) So I guess this still applies: Or at least they make it pretty easy for Open Source projects to use their services without incurring too much of a cost penalty. I’ll follow up on this post to describe how we wrapped this all up in a neat VSTS pipeline to deploy continuously.","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://toonen.io/categories/Cloud/"}],"tags":[{"name":"OpenSource","slug":"OpenSource","permalink":"https://toonen.io/tags/OpenSource/"},{"name":"Cloud","slug":"Cloud","permalink":"https://toonen.io/tags/Cloud/"},{"name":"Stryker","slug":"Stryker","permalink":"https://toonen.io/tags/Stryker/"}],"keywords":[{"name":"Cloud","slug":"Cloud","permalink":"https://toonen.io/categories/Cloud/"}]},{"title":"Bank account in minutes","slug":"Bank-account-in-minutes","date":"2018-03-01T12:29:46.000Z","updated":"2018-03-05T20:07:07.000Z","comments":true,"path":"2018/03/01/Bank-account-in-minutes/","link":"","permalink":"https://toonen.io/2018/03/01/Bank-account-in-minutes/","excerpt":"","text":"","categories":[],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://toonen.io/tags/Windows/"},{"name":"Linux","slug":"Linux","permalink":"https://toonen.io/tags/Linux/"},{"name":"Cross-Platform","slug":"Cross-Platform","permalink":"https://toonen.io/tags/Cross-Platform/"},{"name":"Bash","slug":"Bash","permalink":"https://toonen.io/tags/Bash/"},{"name":"Docker","slug":"Docker","permalink":"https://toonen.io/tags/Docker/"}],"keywords":[]},{"title":"Running Docker 17.10 on Windows Server 1709 without nested virtualization","slug":"running-docker-17-10-on-windows-server-2017-without-nested-virtualization","date":"2018-02-26T11:01:29.000Z","updated":"2018-02-27T21:07:43.000Z","comments":true,"path":"2018/02/26/running-docker-17-10-on-windows-server-2017-without-nested-virtualization/","link":"","permalink":"https://toonen.io/2018/02/26/running-docker-17-10-on-windows-server-2017-without-nested-virtualization/","excerpt":"","text":"Although some people have overheard me saying that Containers are Dead, there is actually some use for it when dealing with legacy software and/or on-premise/cloud hybrid applications.Recently, during a hackathon, I tried to use Docker Swarm’s awesome Routing Mesh but couldn’t get it to work on Windows Server 2016. It turns out that it will only work on Windows Server 1709. Installing Docker EE PreviewSince it was a hackathon anyway, I figured I might as well try to roll Windows Server 1709 (this is the new semi-annual release channel by the way) on a VM and I followed the instructions to install Docker on it. But… it still didn’t work. Turns out, I needed different instructions and the preview. But for some reason it wouldn’t install! It told me I needed to install the Hyper-V feature: Forcing it to run anywayUsing the following piece of Powershell, it’s quite easy to get it to work anyway. This does assume you followed the ‘normal’ installation instructions first though. 1234567891011121314151617181920# Stop DockerStop-Service Docker# Get DockerSave-package -providername DockerProvider -Name Docker -RequiredVersion Preview -Path $Env:TEMP$dockerZipPath = (Resolve-Path $Env:TEMP\\Docker*.zip)Expand-archive $dockerZipPath $Env:TEMP\\Docker# Move to correct locationMove-Item $Env:TEMP\\Docker\\docker\\dockerd.exe \"$Env:ProgramFiles\\Docker\" -forceMove-Item $Env:TEMP\\Docker\\docker\\docker.exe \"$Env:ProgramFiles\\Docker\" -force# Disable Linux containers[Environment]::SetEnvironmentVariable(\"LCOW_SUPPORTED\", $null, \"Machine\")Start-Service Docker# CleanupRemove-Item $dockerZipPath -ForceRemove-Item $Env:TEMP\\Docker -Recurse -Force The LCOW_SUPPORTED environment variable makes sure you won’t accidently try to run a Linux container anyway :-) O, did I forget to mention that? Docker 17.10 adds support for Linux containers on Windows Server through LinuxKit.","categories":[{"name":"Containers","slug":"Containers","permalink":"https://toonen.io/categories/Containers/"}],"tags":[{"name":"Containers","slug":"Containers","permalink":"https://toonen.io/tags/Containers/"},{"name":"Servers","slug":"Servers","permalink":"https://toonen.io/tags/Servers/"}],"keywords":[{"name":"Containers","slug":"Containers","permalink":"https://toonen.io/categories/Containers/"}]},{"title":"Continuous Delivery of Azure Functions with TFS","slug":"continuous-delivery-of-azure-functions-with-tfs","date":"2017-02-24T18:03:23.000Z","updated":"2017-02-24T21:57:47.000Z","comments":true,"path":"2017/02/24/continuous-delivery-of-azure-functions-with-tfs/","link":"","permalink":"https://toonen.io/2017/02/24/continuous-delivery-of-azure-functions-with-tfs/","excerpt":"","text":"In my previous post (Going Serverless - Azure Functions put to use), I showed you how to create a simple serverless app that did some basic alerting based on events on an Azure Service Bus. Now although this example did show you how to create the function and successfully run it, it didn’t show you how to do it properly: by rubbing some DevOps on it. The code I used before was simple enough to maintain but I can imagine you would want to use Visual Studio to develop your functions. Luckily there’s an extension for that. After you’ve installed the extension (make sure to get the updated one and heed the prerequisites), you will be able to create a new Function App quite easily and although it’s not as complete as the docker integration (yet), you can use it to deploy your functions using web deploy rather than the source control integration from the portal. Creating the AppIn Visual Studio create a new solution using the wizard by selecting the (C# -&gt; ) ‘Cloud’ -&gt; ‘Azure Functions’ project type. You will see a project structure very similar to what you’re used to from other project types. It will feature a few files: host.json - contains global config for all the functions within your project. appsettings.json - this is pretty self-explanatory, right? ProjectReadme.html - you can safely remove this. Now as you may have noticed, there’s no actual function yet. You still have to add it by right-clicking the project-node and selecting the ‘Add’ -&gt; ‘New Azure Function’ option. Pick the ‘ServiceBusTopicTrigger - C#’ type and enter the parameters like before. You will notice that after creating the functions, you’ll end up with what we have before, including the project.json we had to manually create in the portal. That also means we can just reuse the code from before :-) Take a look at your function.json file and notice that it has a green squiggly underneath the manage permissions (which we have to use, remember?), I didn’t actually test it with the capital ‘M’ there, but I changed it to ‘manage’ before publishing. Let me know if you do try and succeed!Unfortunately, Visual Studio doesn’t understand this project type completely just yet, so adding NuGet packages is a manual process. You’ll also notice that IntelliSense is limited, it’ll work just fine if you’re using the assemblies which you get out-of-the-box, but if you use external references, I have found it to be lacking. Why use Visual Studio at all?By now you might be wondering what the advantage of using Visual Studio is over just creating a function in the portal. Well, there are several reasons: You might want to store your sources in source control and you’re using TFS - which is not supported in the portal. You might want to create more complex solutions, where you share code over functions for instance. You can do this by adding an empty function and loading it in another by using the #load &quot;..\\shared\\shared.csx&quot; directive at the top of your file (below the #R directives). You can debug your functions. The first time you’ll try this, you will be prompted to download the Azure Functions CLI. So read on if you want to see how to deploy this from source control. TFSI want my release to inject some variables using Guillaume’s replace token build task, then package and publish it. Seeing as a function isn’t really something that you’ll build, it’s rather strange that you’ll need a build to feed your release definition, so you might consider a build definition which directly deploys your function to an Azure Web Application, this won’t allow you to use environments though and because functions don’t support application slots yet, I like using a staging environment before going to production. Whichever way you’ll go, you will have to know that a web deploy is the only possible way to deliver your function to the cloud now.I will assume that you have created a web application and/or build definition before, so I won’t go into that and assume that it’s all in place. My build simply copies all files to a file container on the server, nothing special there. My release definition contains 4 steps per environment: Replace Tokens: replaces all tokens with the correct servicebus topics, the email address, etc. Archive Files: zip the $(System.DefaultWorkingDirectory)/AwesomeNotifier/AwesomeNotifier folder and create a zip-file with $(System.DefaultWorkingDirectory)/package.zip as name. Deploy Azure App Service: select your subscription, the app name and tell it which package to use ($(System.DefaultWorkingDirectory)/package.zip in our case). Azure App Service Manage: select your subscription, select the start method, and select the application. Now if you set the trigger of your build to Continuous Integration and automatically create a release and deploy your (test) environment after a succesful build, you’ll have created a working continuous delivery pipeline to update your Azure Function using Visual Studio and TFS. Good luck!","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://toonen.io/categories/DevOps/"}],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://toonen.io/tags/Azure/"},{"name":"Serverless","slug":"Serverless","permalink":"https://toonen.io/tags/Serverless/"},{"name":"Continuous Delivery","slug":"Continuous-Delivery","permalink":"https://toonen.io/tags/Continuous-Delivery/"},{"name":"DevOps","slug":"DevOps","permalink":"https://toonen.io/tags/DevOps/"},{"name":"TFS","slug":"TFS","permalink":"https://toonen.io/tags/TFS/"}],"keywords":[{"name":"DevOps","slug":"DevOps","permalink":"https://toonen.io/categories/DevOps/"}]},{"title":"Going Serverless - Azure Functions put to use","slug":"azure-functions-put-to-use","date":"2017-02-08T20:16:27.000Z","updated":"2017-02-11T11:43:13.000Z","comments":true,"path":"2017/02/08/azure-functions-put-to-use/","link":"","permalink":"https://toonen.io/2017/02/08/azure-functions-put-to-use/","excerpt":"","text":"We run an application which is event-driven and utilizes microservices across several trust boundaries. The application originated from our ‘automate everything you do more than twice’-mantra and is now continuously evolving and making our live as a small DevOps team easier. The underlying messaging mechanism of our app is an Azure Service Bus (or actually, multiple buses), with several topics and subscriptions upon those topics. As all of our events flow through Azure already, it’s easy to store them in blobstorage and use them for auditing/analysis/what-have-you at a later point in time. Now that the usage is increasing, we felt that it was time to add some alerting and we made plans for a new service that would react to our ‘ActivityFailed’-event, it would then send an email as soon as one of those events (luckily they don’t occur that often) would occur. Sounds easy enough, right? Dockerize or … ?As you may know Docker is a great tool to envelope your application into a well-known and well-described format so that it can run anywhere the same as it would on your machine. We would develop the service in .NET Core, so it would be easy enough to Dockerize it and host it somewhere just like some of the other services. But last night I thought to myself ‘Wait, we run in Azure, use the Azure Service Bus and only need to react to messages on the bus..’ and I decided I would try to create an Azure Function to react to the event and send me the mail. It literally took me about 15 minutes to develop. I’ll describe the process below. Going serverlessAzure Functions are a way to process events in an easy way without having to worry about where you run it. It’s basically ‘just code’ and Azure does the rest for you. I had played with Azure Functions before, but didn’t really find a use-case for it. I do however feel that they are the next step after containerization. It may not fit all problems, but there are certainly use-cases out there which would benefit from a completely serverless architecture. Step one is going to the Azure Portal and creating a new ‘Function App’. Tip: use a consumption plan if you only want to be billed for your actual usage. Once your Function App is created, navigate to it. The first time you navigate to your Function App, you won’t have any functions yet, so you will be presented with the Quickstart Wizard. We will not use it, so scroll down and click ‘Create your own custom function’. Now from the template gallery, select C# as language and ‘Data Processing’ as scenario. Click the ‘ServiceBusTopicTrigger-CSharp’ template and enter the following values in the corresponding fields: Name: a meaningful name for your function, pick something like ‘EmailNotifier’ Topic name: this is the name of the topic on your service bus which you’ll listen to Subscription name: The subscription name on top of the topic specified above Access Rights: select ‘Manage’, and make this match the SAS Token. As of writing this post, there’s a bug preventing you from using the expected ‘Listen’ permissions. That is - you can use it, but your function will cease to trigger after a few hours. Service Bus connection: Service Bus connection strings are saved as Application Setting for your entire Function App and can be shared over multiple functions. Just click ‘new’ the first time and enter the connection string without the EntityPath in it You will now have a basic function. Congratulations! Making it do something usefulIn order to do something meaningful with our app, we’ll need to go through a few steps. First let’s discover what is created for us. Click the ‘Files’ button on the top right of the editor: You will see that you have two files: function.json - which describes your in- and outputs run.csx - which is the code for your function Take some time to familiarize you with both files and notice that the run.csx isn’t much different from a regular C# program. It actually has using statements and a public static void Main() alike function called ‘Run’. Azure Functions provides you with framework libraries such as System and System.Linq and you can include some additional assemblies using the #r directive. A full list of all available assemblies can be found here. As you can see, using all types/methods within the Microsoft.ServiceBus namespace will be easy. I can just add a the following lines of code to the beginning of run.csx: 123#r &quot;Microsoft.ServiceBus&quot;using Microsoft.Servicebus; I also will be using Newtonsoft.Json to deserialize my messages and SendGrid to send my emails, so I will need some way to restore the NuGet packages. This turns out to be quite easy. I just have to add a new file and tell my function what my dependencies are. Add a file called project.json to your function like so: Now add the following code to it: 12345678910&#123; &quot;frameworks&quot;: &#123; &quot;net46&quot;:&#123; &quot;dependencies&quot;: &#123; &quot;Sendgrid&quot;: &quot;8.0.5&quot;, &quot;Newtonsoft.Json&quot;: &quot;9.0.1&quot; &#125; &#125; &#125;&#125; This will trigger my function to perform a NuGet restore before executing my function for the first time. Don’t forget to add the using statements to your code. We’re almost ready to get the code done but first we’ll need to add an output to our function. Head to the ‘Integrate’ section of your function and take note of the ‘Message parameter name’, we will use this later on. Now click ‘New Output’ and select ‘SendGrid’ (currently in preview). The easiest way to utilize this output, is to enter the from, to, subject and API key here. Mind you that the API key is the name of an Application Setting which contains the actual key! Save the changes and then add the Application Setting corresponding to the API key name (SendGridApiKey in this example) by clicking ‘Function App Settings’ and then ‘Configure app setings’Once you’ve added the input, take a look at your function.json and see how it reflects the changes. Finally adjust the code for run.csx to reflect your application logic. Notice how I named the ‘Message parameter name’ incomingMessage and added an out Mail message to the method signature: 12345678910111213141516171819202122232425262728#r &quot;SendGrid&quot;#r &quot;Newtonsoft.Json&quot;#r &quot;Microsoft.ServiceBus&quot;using SendGrid.Helpers.Mail;using Newtonsoft.Json;using System;using System.Threading.Tasks;using Microsoft.ServiceBus.Messaging;public static void Run(BrokeredMessage incomingMessage, TraceWriter log, out Mail message)&#123; message = null; // set output to null, it must be set as it is a mandatory out parameter var msgBody = incomingMessage.GetBody&lt;string&gt;(); var msg = JsonConvert.DeserializeObject&lt;dynamic&gt;(msgBody); log.Info($&quot;Event type: &#123;msg.messageType&#125;&quot;); if(msg.messageType == &quot;activityFailed&quot;) &#123; log.Info($&quot;Found a failed activity: &#123;msg.processId&#125;&quot;); message = new Mail(); var messageContent = new Content(&quot;text/html&quot;, $&quot;Activity Failed: &#123;msg.processId&#125;&quot;); message.AddContent(messageContent); &#125;&#125; That’s it. Click Run and your message will be parsed, checked and you will be alerted in case something goes wrong :-) The resultI’ve already received my first alert - even though I triggered it intentionally, it’s still awesome to see that I now have a low-cost, easy to use solution which only runs when it should. Of course there optimizations to be made, but for now it does the trick. And in the meanwhile I’ve learned some more about building serverless applications using Azure Functions.","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://toonen.io/categories/Cloud/"}],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://toonen.io/tags/Azure/"},{"name":"Serverless","slug":"Serverless","permalink":"https://toonen.io/tags/Serverless/"}],"keywords":[{"name":"Cloud","slug":"Cloud","permalink":"https://toonen.io/categories/Cloud/"}]},{"title":"Git in VS2017 with self-signed SSL","slug":"git-in-vs2017-with-self-signed-ssl","date":"2016-11-28T11:00:47.000Z","updated":"2017-02-08T22:53:51.000Z","comments":true,"path":"2016/11/28/git-in-vs2017-with-self-signed-ssl/","link":"","permalink":"https://toonen.io/2016/11/28/git-in-vs2017-with-self-signed-ssl/","excerpt":"","text":"When I’m out of the office, I connect to my team’s TFS server through the firewall and get served up with a properly signed (by a widely trusted CA) SSL certificate.This means that my browser, and git have no issues connecting and cloning. When I’m in the office and connected to our corporate WiFi network, I get a self-signed SSL certificate. It’s always been a hassle to add these certificates to Git’s local certificate store but luckily Visual Studio didn’t require you to do the same, seeing as they used Lib2Git. With VS2017, Microsoft switched to git.exe (which is good) but they aren’t using the one already on your path but rather a bundled installation which resides in the VS2017 extensions directory. This means that you have to add SSL certificates to yet another git trusted store. Let’s fixMicrosoft has done a https://blogs.msdn.microsoft.com/phkelley/2014/01/20/adding-a-corporate-or-self-signed-certificate-authority-to-git-exes-store/ of how to add a certificates should be added to your git.exe client and now this must be applied to Visual Studio as well to prevent this from happening: The Git client resides in your VS2017 installation dir, which by default is C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\. Now if you browse to your edition (i.e. ‘Enterprise’), you will see the familiar Common7\\IDE directory and then to the CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer\\Git\\mingw32\\ssl\\certs folder, you will find the ca-bundle.crt that Visual Studio uses. So the full path (for a default installation of VS2017 Enterprise) would be: C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer\\Git\\mingw32\\ssl\\certs Add your Base64 encoded certificate and the next time you attempt to clone a repo within VS2017, you should be presented with the trusted VS logo ASCII art from TFS: Hope this saves you a bit of trouble ;-)","categories":[],"tags":[{"name":"Visual Studio","slug":"Visual-Studio","permalink":"https://toonen.io/tags/Visual-Studio/"}],"keywords":[]},{"title":"Coretainers","slug":"coretainers","date":"2016-11-16T19:56:02.000Z","updated":"2017-02-08T22:53:23.000Z","comments":true,"path":"2016/11/16/coretainers/","link":"","permalink":"https://toonen.io/2016/11/16/coretainers/","excerpt":"","text":"Most people, if not everyone, have seen the .NET Core demo’s in a Docker container on Linux by now. Some may even have experimented with Windows containers and the full fledged .NET framework as I showed at the SDN Event in September.The thing is, that if you haven’t looked at containers by now, you’re in for a treat. Where it used to be quite hard to figure everything out for yourself, Microsoft announced a new way of integrating today and are taking it to the next level in Visual Studio 2017. Especially when you combine the power of containers with the flexibility of .NET Core. Docker made easyThe combination of .NET Core and containers is very powerful. It gives a small iamge, which runs anywhere. You can literally ship your ‘machine’ and today it became even easier.Starting with Visual Studio 2017, when you create a web application, you can enable Docker support from the box: If you have Docker for Windows installed, you can get going. If not, install it first.This will automatically generate several files for you: Dockerfile (where it all starts) docker-compose.yml (compose your containers, more on this in a future post) docker-compose.ci.build.yml (instructions for a CI build) This will be all you need to get going. Really, that’s it. Just press ‘F5’ (or click the debug button, which now conventiently says ‘Docker’).Visual Studio will now start building your application and put it into a container. The best part here is that it will link your source files on disk into the container by using volumes. If you inspect the docker-compose.vs.debug.yml file, you can clearly see the line that says: - .:/app what this line does, is that it links the current directory to the /app directory within the container. This means you can edit your code (and views) live, refresh your browser and it’ll update the app that you’re running within the container. The best thing is though, you can set breakpoints and they work just as though it was an application running on your local dev machine. Mind you: if your debug experience didn’t go quite as planned and you run into an error. You might just see something like this in the output window: ERROR: for awesomewebapp Cannot create container for service awesomewebapp: D: drive is not shared. Please share it in Docker for Windows Settings Although the error message is quite verbose nowadays, right-click the Docker icon in your taskbar and go to settings. Now on the ‘Shared Drives’ tab, you can share the disk where your application resides. Publish to AzureNow where it get’s really awesome, is that starting today you can publish your container to Azure with a few simple clicks. If you right-click your project, you can press ‘Publish’. We all know this action from years of publishing web applications through WebDeploy - and we all know what joy that brought ;-)We then got the ability to quickly select ‘host in Azure’ when we created the project and now we have this: The settings are simple: Provide a unique name for your app Select an Azure Subscription Select a resource group, or create one Select or create an App Service Plan Select or create a Docker registry I’m assuming you’re familiar with Azure terms such as the resource group and service plan, but the last one deserves a bit of explanation. A Docker registry is like a repository where your containers are stored. You can have both private and public registries - DockerHub being the most famous one. By default this will create a private registry where you can store the different versions of your container. Press the ‘create’ button. Visual Studio and Azure will do the rest for you, it’s that simple. Mind you: make sure that both your app service plan and registry are in the same Azure region. As of writing this post, only West US is supported. You can select the region from the ‘Services’ tab and then pressing the gears next to the app service or registry you’re creating. ResultAfter pushing the ‘create’ button, my container got published to Azure and I’m able to access it from my browser. And although this is of course an awesome way to publish your application, this is probably not what you want from a DevOps perspective. You want to be able to make a change to the app, commit and push your changes to the repo and have an automated build/release pipeline to put your changes in production… and you can!That’s what another new option in VS2017 does for you: More on this feature in a later post though. For now, experiment with the containers and new features you have and I’ll show you how to automatically create a CI/CD pipeline from right within Visual Studio in a future post.","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://toonen.io/categories/DevOps/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://toonen.io/tags/Docker/"},{"name":"DevOps","slug":"DevOps","permalink":"https://toonen.io/tags/DevOps/"},{"name":"Connect","slug":"Connect","permalink":"https://toonen.io/tags/Connect/"},{"name":"Containers","slug":"Containers","permalink":"https://toonen.io/tags/Containers/"}],"keywords":[{"name":"DevOps","slug":"DevOps","permalink":"https://toonen.io/categories/DevOps/"}]},{"title":"New Blog","slug":"new-blog","date":"2016-07-09T21:09:34.000Z","updated":"2016-07-09T23:18:54.000Z","comments":true,"path":"2016/07/09/new-blog/","link":"","permalink":"https://toonen.io/2016/07/09/new-blog/","excerpt":"","text":"So as you may have noticed, I have started a new blog. It’s been a long time coming but I finally found some time this weekend. My colleague Edwin van Wijk tipped me off on using hexo quite a while ago and I seem to have gotten the hang of it. This blog itself is still a work in progress and I’ll be migrating old posts over soon, but in the meanwhile I figured I’d share some tips. Free BlogAs you might know, GitHub offers you a free website through GitHub Pages. This means that you can host your static website right from GitHub. Combine this with Hexo magic and you can start your own blog quite easily. What you might not know is that you can also add a custom domain to your GitHub page: Now although this by itself is pretty cool, it gets better. Although it’s possible to use SSL on GitHub pages, this isn’t currently possible when using a custom domain, or is it? CloudFlare to the rescueCloudFlare offers a free tier that not only makes your website faster by using a smart caching mechanism (which you might want to turn off seeing as hexo generates static content), it also offers free SSL for all sites. Simply register for a free account on their site, go to the ‘DNS’ tab and add a CNAME for your domain, like so: For the DNS-savvy, yes, I used a CNAME as my domain’s root, please refer to this page on details as to why this is still RFC compliant. Then nagivate to the ‘Crypto’ tab in the menu and set it to the following: Now for the final step, which ensures all your users are automatically redirected to your SSL page, navigate to the ‘Page Rules’ tab and add the following rules (where you replace the domain with your own domain). If you use a sub-domain such as ‘blog.domain.com’, make sure to use two asterisks (*) in the first rule and replace $1 in the rule with $2 so that it will correctly rewrite: In case you do want to disable caching to prevent issues with your static site, enable a third rule where you match https://yourdomain.ext/* and set the action to ‘Cache Level = ByPass’: Sit back and relaxThat’s it. You’re done. You have just setup your new secure site using hexo, GitHub pages and CloudFlare. Of course you can also use this with the Basic Tier in Azure which allows you to use your own custom SSL for just 8 odd euro’s a month ;-)","categories":[{"name":"General","slug":"General","permalink":"https://toonen.io/categories/General/"}],"tags":[{"name":"tips-and-tricks","slug":"tips-and-tricks","permalink":"https://toonen.io/tags/tips-and-tricks/"},{"name":"freebies","slug":"freebies","permalink":"https://toonen.io/tags/freebies/"}],"keywords":[{"name":"General","slug":"General","permalink":"https://toonen.io/categories/General/"}]},{"title":"Bash for Windows","slug":"bash-for-windows","date":"2016-04-06T20:59:00.000Z","updated":"2016-07-09T23:14:52.000Z","comments":true,"path":"2016/04/06/bash-for-windows/","link":"","permalink":"https://toonen.io/2016/04/06/bash-for-windows/","excerpt":"","text":"So last week at //Build/ Microsoft announced native Bash-integration on the Windows 10 platform and today they delivered the first preview. Being a Windows Insider since nearly day 1 – including installing those buggy mobile builds on my daily driver – I still have my daily driver set to the fast ring and I received build 14316 today. After about 30 mins of installation (ymmv), I eagerly logged in and typed ‘bash’. Unfortunately, nothing happened. Then I realized I had to switch some options on. First you need to enable the ‘developer mode’. You can do this by opening the settings app and selecting the correct option: Next you can enable the optional windows feature ‘Windows Subsystem for Linux (Beta)’: After a reboot, you can press the windows key and enter ‘bash’. A new prompt will open with the question if you want to install Ubuntu – say what: And that’s it, you’re root: A few tips: right click the title bar and go to ‘properties’ enable ‘quick editing’ here, this allows you to copy/paste into the window. if you’re like me, and you try to install Docker even though you kind of knew it wouldn’t work: it doesn’t work. Luckily there’s an easy integration running a docker host in HyperV just around the corner (and I run the beta already), so no sweat there, just had to try 🙂","categories":[{"name":"General","slug":"General","permalink":"https://toonen.io/categories/General/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://toonen.io/tags/Windows/"},{"name":"Linux","slug":"Linux","permalink":"https://toonen.io/tags/Linux/"},{"name":"Cross-Platform","slug":"Cross-Platform","permalink":"https://toonen.io/tags/Cross-Platform/"},{"name":"Bash","slug":"Bash","permalink":"https://toonen.io/tags/Bash/"},{"name":"Docker","slug":"Docker","permalink":"https://toonen.io/tags/Docker/"}],"keywords":[{"name":"General","slug":"General","permalink":"https://toonen.io/categories/General/"}]},{"title":"Dev Intersection 2015 - dag 6","slug":"dev-intersection-2015-dag-6","date":"2015-11-09T17:01:00.000Z","updated":"2016-07-09T23:18:10.000Z","comments":true,"path":"2015/11/09/dev-intersection-2015-dag-6/","link":"","permalink":"https://toonen.io/2015/11/09/dev-intersection-2015-dag-6/","excerpt":"","text":"Ook al ben ik inmiddels alweer een tijdje terug uit Las Vegas en inmiddels de jet-lag te boven, wilde ik jullie toch niet mijn laatste dag op Dev Intersection onthouden. Dit was namelijk de dag waar ik het meest naar had uitgekeken. Ondanks mijn eerdere experimenten met IoT (dotnetFlix aflevering 3) waarbij ik mijn gas- en electriciteitsmeter liet ‘praten’ met het internet en wat andere simpele projectjes, had ik nog steeds niet echt het idee dat ik met IoT bezig was, zo had ik wel een koppeling met Azure gemaakt maar niet de IoT hub gebruikt, geen stream analytics toegepast en geen PowerBI. Dat was nou precies waar mijn laatste workshop over ging: IoT, Azure, PowerBI en dat aangestuurd met Node.js! Gedurende de gehele dag werd ik meegenomen door Doug Seven en zijn team waarbij we in eerste instantie aan de slag gingen met een Particle Photon. Deze mini-module van $19 (zie foto waar hij bovenop een breadboard ligt) is in staat om out-of-the-box te communiceren met wifi en heeft een aantal digitale en analoge poorten aan boord waarmee je kunt communiceren. Plug ‘m in in je PC (of een andere USB power source) en je kunt gaan zodra je jouw particle hebt ‘geclaimt’ via hun cloud service. Tijdens de workshop wordt uitgelegd dat je op verschillende manieren om kunt gaan met je devices, zo kun je rechtstreeks met het internet communiceren, of je kunt via een gateway-device werken. Zo doen wij dat ook deze dag: via onze pc. Gewapend met een text-editor (ik koos voor Visual Studio Code), de Johnny Five node module en de Particle-cli module, kon ik aan de slag met Node.js. Aangezien er geen ‘hello world’ te outputten was op de module aangezien er geen display op zit, moest een knipperend lampje het doen (dat mijn lampje in morse alsnog ‘hello world’ seinde, laten we maar even buiten beschouwing ;-)). Probeer overigens ook vooral het particle-cli commando ‘nyan’ en ik geef je alvast als tip dat je ook ‘particle-cli nyan off’ kunt doen zonder een reboot te geven. Gedurende de dag kwamen we steeds verder met onze particles en koppelden we deze aan een SparkFun weathershield waarmee een simpel weerstation werd gebouwd. Door deze metrieken vervolgens met behulp van de Azure IoT node-module naar Azure te pushen en deze met een stream analytics job in een Power BI DataSet te gieten, kun je in Power BI vervolgens een mooi dashboard er overheen gieten. Let er hierbij op dat je om PowerBI als output voor je Stream Analytics Job te selecteren, je in de oude huidige Azure portal moet kijken! Zie hieronder mijn resultaat met op de horizontale as de tijd en verticaal de temperatuur :-) Al met al was dit een leerzame workshop waar je in korte tijd met een hoop informatie tot je neemt, en je de kans krijgt te werken met de mannen die hier achter zitten en ze vragen te stellen. Krijg je dus de kans om een workshop van Doug en de mannen te volgen: grijp ‘m! Kijk op hun github voor de code, guides en workshop planning.","categories":[{"name":"Conferences","slug":"Conferences","permalink":"https://toonen.io/categories/Conferences/"}],"tags":[{"name":"DevInterSection","slug":"DevInterSection","permalink":"https://toonen.io/tags/DevInterSection/"},{"name":"Dutch","slug":"Dutch","permalink":"https://toonen.io/tags/Dutch/"}],"keywords":[{"name":"Conferences","slug":"Conferences","permalink":"https://toonen.io/categories/Conferences/"}]},{"title":"Dev Intersection 2015 - dag 3","slug":"dev-intersection-2015-dag-3","date":"2015-10-28T06:38:00.000Z","updated":"2016-07-09T23:18:22.000Z","comments":true,"path":"2015/10/27/dev-intersection-2015-dag-3/","link":"","permalink":"https://toonen.io/2015/10/27/dev-intersection-2015-dag-3/","excerpt":"","text":"Samen met Mark Rexwinkel en Edwin van Wijk ben ik deze week aanwezig op de Dev Intersection 2015 conferentie in Las Vegas. Via een dagelijkse blog proberen wij jullie op de hoogte te houden van wat we hier zien en horen. Na Edwin en Mark ben ik vandaag aan de beurt. De derde dag van de conferentie was de eerste dag waarop ‘reguliere’ sessies werden gegeven. Na een goed ontbijt, begon de dag met een keynote van Scott Guthrie. Hij vertelde voornamelijk over de ‘Journey to the Cloud’ en deelde Microsoft’s visie op DevOps met Visual Studio Online, enkele indrukwekkende cijfers over Azure (wat te denken van 777 biljoen storage queries per dag?!) en de manier waarop Microsoft’s Clouddiensten zoals Office 365 in het grote plaatje van de moderne IT-industrie passen. Na de keynote zijn we elk onze eigen kant uit gegaan. Ik heb sessies gevolgd van Julie Lerman (Domain Driven Design for the Database Driven Mind), welke erg goed wordt samengevat in een drietal blogposts, een sessie van Steven Murawski (Survive and Thrive in a DevOps World) die erop neer kwam dat het invoeren van DevOps voornamelijk een cultuur-shift is waarbij men de ‘fear culture’ en ‘blame game’ moet laten varen. Hij heeft een flink aantal tips op zijn blog staan om met DevOps aan de slag te gaan. In de middag ben ik verder gegaan met een sessie van Troy Hunt (Securing ASP.NET in an Azure Environment). Nadat ik zijn workshop had gevolgd op maandag, was ik erg benieuwd wat hij over dit onderwerp had te zeggen en ik werd niet teleurgesteld. Alhoewel het in het begin voornamelijk om no-brainers ging zoals het least-privileged-account principe, kwam hij uiteindelijk tot tips omtrent dynamic data masking in Azure SQL databases, stipte hij nog even het belang van application settings en connection strings in de Azure portal aan en dat je eigenlijk altijd two step verification aan moet zetten als je met jouw account een Azure subscription gaat beheren. Dit laatste kun je instellen via accountbeheer. Al met al was dit weer een geslaagde dag en kijk ik al uit naar morgen!","categories":[{"name":"Conferences","slug":"Conferences","permalink":"https://toonen.io/categories/Conferences/"}],"tags":[{"name":"DevInterSection","slug":"DevInterSection","permalink":"https://toonen.io/tags/DevInterSection/"},{"name":"Dutch","slug":"Dutch","permalink":"https://toonen.io/tags/Dutch/"}],"keywords":[{"name":"Conferences","slug":"Conferences","permalink":"https://toonen.io/categories/Conferences/"}]},{"title":"Custom build tasks in TFS 2015","slug":"custom-build-tasks-in-tfs-2015","date":"2015-07-21T12:46:00.000Z","updated":"2016-07-09T23:19:22.000Z","comments":true,"path":"2015/07/21/custom-build-tasks-in-tfs-2015/","link":"","permalink":"https://toonen.io/2015/07/21/custom-build-tasks-in-tfs-2015/","excerpt":"","text":"Since I upgraded my team’s private TFS instance to TFS 2015 RC1, followed by RC2, the whole team has been working with TFS 2015 quite a lot. Of course one of the major features is the new build engine and we’ve given that quite a ride. From cross platform builds on Mac and Linux to custom build tasks, we’ve accomplished quite a lot. Seeing as during yesterday’s Visual Studio 2015 launch, Brian Harry stated that it was ‘quite easy’ to build your own tasks, I figured I’d give a short write-down of our experiences with custom tasks. PrefaceFrom the moment I upgraded our R&amp;D server to RC1, we’ve been working with the new build system. Up until RC2 it was only possible to add custom build tasks, but we weren’t able to remove them. On top of that, the whole process isn’t documented quite yet. Seeing as we quite often add NuGet packages to a feed and didn’t want to add a, not very descriptive, PowerShell task to all of our build definitions, we decided to use this example for a custom task and see how it would fare. Prerequisite one: What is a task?To make a custom build task, we first need to know what it looks like. Luckily Microsoft has open-sourced most of the current build tasks in https://github.com/Microsoft/vso-agent-tasks which gave us a fair idea of what a build task is: a JSON file describing the plugin a PowerShell or Node.JS file containing the functionality (this post will focus on PowerShell) an (optional) icon file optional resources translating the options to another language Now the only thing we needed to find out was: how to upload these tasks and in what format? Good to know: To make sure your icon displays correctly, it must be 32×32 pixels The task ID is a GUID which you need to create yourself The task category should be an existing category Visibility tells you what kind of task it is, possible values are: Build, Release and Preview. Currently only Build-type tasks are shown Prerequisite two: How to upload a task?We quickly figured out that the tasks were simply .zip files containing the aforementioned items, so creating a zip was an easy but then we needed to get it there. By going through the github repository’s, we figured out there was a REST-API which controls all the tasks and we figured that by doing a PUT-call to said endpoint we could create a new task, but also overwrite tasks. The following powershell-script enables you to upload tasks: 12345678910111213141516171819202122232425262728293031param( [Parameter(Mandatory=$true)][string]$TaskPath, [Parameter(Mandatory=$true)][string]$TfsUrl, [PSCredential]$Credential = (Get-Credential), [switch]$Overwrite = $false)# Load task definition from the JSON file$taskDefinition = (Get-Content $taskPath\\task.json) -join \"`n\" | ConvertFrom-Json$taskFolder = Get-Item $TaskPath# Zip the task contentWrite-Output \"Zipping task content\"$taskZip = (\"&#123;0&#125;\\..\\&#123;1&#125;.zip\" -f $taskFolder, $taskDefinition.id)if (Test-Path $taskZip) &#123; Remove-Item $taskZip &#125;Add-Type -AssemblyName \"System.IO.Compression.FileSystem\"[IO.Compression.ZipFile]::CreateFromDirectory($taskFolder, $taskZip)# Prepare to upload the taskWrite-Output \"Uploading task content\"$headers = @&#123; \"Accept\" = \"application/json; api-version=2.0-preview\"; \"X-TFS-FedAuthRedirect\" = \"Suppress\" &#125;$taskZipItem = Get-Item $taskZip$headers.Add(\"Content-Range\", \"bytes 0-$($taskZipItem.Length - 1)/$($taskZipItem.Length)\")$url = (\"&#123;0&#125;/_apis/distributedtask/tasks/&#123;1&#125;\" -f $TfsUrl, $taskDefinition.id)if ($Overwrite) &#123; $url += \"?overwrite=true\"&#125;# Actually upload itInvoke-RestMethod -Uri $url -Credential $Credential -Headers $headers -ContentType application/octet-stream -Method Put -InFile $taskZipItem Good to know: Currently only ‘Agent Pool Administrators’ are able to add/update or remove tasks. Tasks are server-wide, this means that you will upload to the server, not to a specific collection or project. Creating the actual taskSo like I said, we’ll be creating a new task that’s going to publish our NuGet packages to a feed. So first we need to decide what information we need to push our packages: The target we want to pack (.csproj or .nuspec file relative to the source-directory) The package source we want to push to For this example I’m assuming you’re only building for a single build configuration and single target platform, which we’ll use in the PowerShell-script. First we’ll make the task definition. As I said, this is simply a JSON file describing the task and its inputs. 12345678910111213141516171819202122232425262728293031323334353637383940&#123; \"id\": \"61ed0e1d-efb7-406e-a42b-80f5d22e6d54\", \"name\": \"NuGetPackAndPush\", \"friendlyName\": \"Nuget Pack and Push\", \"description\": \"Packs your output as NuGet package and pushes it to the specified source.\", \"category\": \"Package\", \"author\": \"Info Support\", \"version\": &#123; \"Major\": 0, \"Minor\": 1, \"Patch\": 0 &#125;, \"minimumAgentVersion\": \"1.83.0\", \"inputs\": [ &#123; \"name\": \"packtarget\", \"type\": \"string\", \"label\": \"Pack target\", \"defaultValue\": \"\", \"required\": true, \"helpMarkDown\": \"Relative path to .csproj or .nuspec file to pack.\" &#125;, &#123; \"name\": \"packagesource\", \"type\": \"string\", \"label\": \"Package Source\", \"defaultValue\": \"\", \"required\": true, \"helpMarkDown\": \"The source we want to push the package to\" &#125; ], \"instanceNameFormat\": \"Nuget Pack and Push $(packtarget)\", \"execution\": &#123; \"PowerShell\": &#123; \"target\": \"$(currentDirectory)\\\\PackAndPush.ps1\", \"argumentFormat\": \"\", \"workingDirectory\": \"$(currentDirectory)\" &#125; &#125;&#125; This version of the task will be a very rudimentary one, which doesn’t do much (any) validation, so you might want to add that yourself. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980[cmdletbinding()]param( [Parameter(Mandatory=$true)][string] $packtarget, [Parameter(Mandatory=$false)][string] $packagesource)##################################################################################################### 1 Auto Configuration##################################################################################################### Stop the script on error$ErrorActionPreference = \"Stop\"# Relative location of nuget.exe to build agent home directory$nugetExecutableRelativePath = \"Agent\\Worker\\Tools\\nuget.exe\"# These variables are provided by TFS$buildAgentHomeDirectory = $env:AGENT_HOMEDIRECTORY$buildSourcesDirectory = $Env:BUILD_SOURCESDIRECTORY$buildStagingDirectory = $Env:BUILD_STAGINGDIRECTORY$buildPlatform = $Env:BUILDPLATFORM$buildConfiguration = $Env:BUILDCONFIGURATION$packagesOutputDirectory = $buildStagingDirectory# Determine full path of pack target file$packTargetFullPath = Join-Path -Path $buildSourcesDirectory -ChildPath $packTarget# Determine full path to nuget.exe$nugetExecutableFullPath = Join-Path -Path $buildAgentHomeDirectory -ChildPath $nugetExecutableRelativePath##################################################################################################### 2 Create package####################################################################################################Write-Host \"2. Creating NuGet package\"$packCommand = (\"pack `\"&#123;0&#125;`\" -OutputDirectory `\"&#123;1&#125;`\" -NonInteractive -Symbols\" -f $packTargetFullPath, $packagesOutputDirectory)if($packTargetFullPath.ToLower().EndsWith(\".csproj\"))&#123; $packCommand += \" -IncludeReferencedProjects\" # Remove spaces from build platform, so 'Any CPU' becomes 'AnyCPU' $packCommand += (\" -Properties `\"Configuration=&#123;0&#125;;Platform=&#123;1&#125;`\"\" -f $buildConfiguration, ($buildPlatform -replace '\\s',''))&#125;Write-Host (\"`tPack command: &#123;0&#125;\" -f $packCommand)Write-Host (\"`tCreating package...\")$packOutput = Invoke-Expression \"&amp;'$nugetExecutableFullPath' $packCommand\" | Out-StringWrite-Host (\"`tPackage successfully created:\")$generatedPackageFullPath = [regex]::match($packOutput,\"Successfully created package '(.+(?&lt;!\\.symbols)\\.nupkg)'\").Groups[1].ValueWrite-Host `t`t$generatedPackageFullPathWrite-Host (\"`tNote: The created package will be available in the drop location.\")Write-Host \"`tOutput from NuGet.exe:\"Write-Host (\"`t`t$packOutput\" -Replace \"`r`n\", \"`r`n`t`t\")##################################################################################################### 3 Publish package####################################################################################################Write-Host \"3. Publish package\"$pushCommand = \"push `\"&#123;0&#125;`\" -Source `\"&#123;1&#125;`\" -NonInteractive\"Write-Host (\"`tPush package '&#123;0&#125;' to '&#123;1&#125;'.\" -f (Split-Path $generatedPackageFullPath -Leaf), $packagesource)$regularPackagePushCommand = ($pushCommand -f $generatedPackageFullPath, $packagesource)Write-Host (\"`tPush command: &#123;0&#125;\" -f $regularPackagePushCommand)Write-Host \"`tPushing...\"$pushOutput = Invoke-Expression \"&amp;'$nugetExecutableFullPath' $regularPackagePushCommand\" | Out-StringWrite-Host \"`tSuccess. Package pushed to source.\"Write-Host \"`tOutput from NuGet.exe:\"Write-Host (\"`t`t$pushOutput\" -Replace \"`r`n\", \"`r`n`t`t\") To finish up, don’t forget to add a .png logo to your task ;-)You should now be able to add a custom task to your build pipeline from the “Package” category: Words of warningTasks can be versioned, use this to your advantage. All build definitions use the latest available version of a specific task, you can’t change this behavior from the web interface, so always assume the latest version is being used. If you don’t change the version number of your task when updating it, the build agents that have previously used your task will not download the newer version because the version number is still the same. This means that if you change the behavior of your task, you should always update the version number! When deleting a task, this task is not automatically removed from current build definitions, on top of that you won’t get a notification when editing the build definition but you will get an exception on executing a build based on that definition. Tasks are always available for the entire TFS instance, this means that you shouldn’t include credentials or anything that you don’t want others to see. Use ‘secret variables’ for this purpose: Further ReadingIf you’ve followed this post so far, I recommend you also check out my team member Jonathan’s post/videos (in Dutch) out: Blog Post about Invoke SQLCmd in build vNextVideo on build vNext (in Dutch)","categories":[{"name":".NET","slug":"NET","permalink":"https://toonen.io/categories/NET/"}],"tags":[{"name":"TFS","slug":"TFS","permalink":"https://toonen.io/tags/TFS/"},{"name":"Build","slug":"Build","permalink":"https://toonen.io/tags/Build/"},{"name":"Customization","slug":"Customization","permalink":"https://toonen.io/tags/Customization/"}],"keywords":[{"name":".NET","slug":"NET","permalink":"https://toonen.io/categories/NET/"}]},{"title":"Load testing from the Azure portal","slug":"load-testing-from-the-azure-portal","date":"2015-07-12T14:57:00.000Z","updated":"2016-07-09T23:18:58.000Z","comments":true,"path":"2015/07/12/load-testing-from-the-azure-portal/","link":"","permalink":"https://toonen.io/2015/07/12/load-testing-from-the-azure-portal/","excerpt":"","text":"Before you launch a new web application, you make sure you have thoroughly tested it, you have performed unit-, integration-, usability- and load-tests but for some reason when the application goes into production, it comes to a grinding halt and you’re left puzzled as to why this happened. Back in 2013 Microsoft released a solution for this issue: Azure-based load testing which is able to simulate real-world load-testing on your application from Azure with unlimited resources (well, the only real limiting factor is your wallet). The only strange thing here was that in order to use this Azure-based load testing, I had to go to my VSO account to start a test instead of just starting a load test in the Azure portal where I published my web application. This has changed now. Introducing Azure load testing from the portalYesterday I stumbled onto this post (which contains way more pictures than this post will) by Charles Sterling, where he revealed that as an ‘nascent feature PM’ he more or less accidentally released a new feature into the wild. As of now it’s possible to start a load test from the Azure portal right from where you control your web application. It’s as easy as adding a tile to your web app and starting the test. Or even better, by enabling a feature flag and simply adding a new load test. To get started, load up your Azure Portal (the new one!) and navigate to one of your web apps and then follow these steps: Right-click the space in between any of the tiles already displayed and click ‘Add Tiles’ Now choose the ‘Operations’ category and select ‘Cloud Load Test’ You will now get a new tile in your web app panel Click ‘Done’ on the top left Click the tile and add a new Load Test, enter the VSO account you want to use, the URL and a name for the test. Mind you, the test name can’t contain any spaces or non-alphanumeric characters. In case you don’t want to add a new tile, you can also include the following feature flag in the portal URL: ?websitesextension_cloudloadtest=true turning the URL into something like: https://portal.azure.com/?websitesextension_cloudloadtest=trueAfter doing so, you will be able to access load testing from your web app’s settings option. SummarizingYou now have a new way to perform load testing in the Azure portal, snugly in your Web App blade. It is currently lacking some of the features that VSO does offer, such as browser distribution and think time, but who knows, they might just add them before the final version: All in all it’s a nice time-saver and the tests are now in a place where I’d actually expect them to be.","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://toonen.io/categories/Cloud/"}],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://toonen.io/tags/Azure/"},{"name":"Test","slug":"Test","permalink":"https://toonen.io/tags/Test/"}],"keywords":[{"name":"Cloud","slug":"Cloud","permalink":"https://toonen.io/categories/Cloud/"}]}]}